---
title: Online Continual Knowledge Learning for Language Models
authors:
- Yuhao Wu*
- Tongjun Shi*
- Karthick Sharma
- Chun Wei Seah
- Shuhao Zhang
date: '2023-11-16'
publishDate: '2024-03-02T14:08:30.656506Z'
publication_types:
- manuscript
publication: '*arXiv*'
abstract: 'Large Language Models (LLMs) serve as repositories of extensive world knowledge, enabling them to perform tasks such as question-answering and fact-checking. However, this knowledge can become obsolete as global contexts change. In this paper, we introduce a novel problem in the realm of continual learning: Online Continual Knowledge Learning (OCKL). This problem formulation aims to manage the dynamic nature of world knowledge in LMs under real-time constraints. We propose a new benchmark and evaluation metric designed to measure both the rate of new knowledge acquisition and the retention of previously learned knowledge. Our empirical evaluation, conducted using a variety of state-of-the-art methods, establishes robust base-lines for OCKL. Our results reveal that existing continual learning approaches are unfortunately insufficient for tackling the unique challenges posed by OCKL. We identify key factors that influence the trade-off between knowledge acquisition and retention, thereby advancing our understanding of how to train LMs in a continually evolving environment.'
tags:
- Computer Science - Computation and Language
- Computer Science - Artificial Intelligence
links:
  - name: arXiv
    url: https://arxiv.org/abs/2311.09632
  - name: PDF
    url: https://arxiv.org/pdf/2311.09632.pdf
  - name: Google Scholar
    url: https://scholar.google.com/scholar?q=Online+Continual+Knowledge+Learning+for+Language+Models
---
