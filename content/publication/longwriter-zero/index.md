---
title: 'LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning'
authors:
- Yuhao Wu*
- Yushi Bai*
- Jiajie Zhang
- Xin Lv
- Roy Ka-Wei Lee
- Juanzi Li
date: '2025-01-01'
publication_types:
- manuscript
publication: 'Submitted to ICLR 2026'
abstract: 'Generating ultra-long coherent text (10,000+ words) remains a significant challenge for large language models. We introduce LongWriter-Zero, a novel approach that employs reinforcement learning to train models for extended text generation without requiring human-annotated long-form data. Our method uses carefully designed reward functions that encourage coherence, structural consistency, and content diversity across long outputs. LongWriter-Zero incorporates hierarchical planning mechanisms and dynamic attention strategies to maintain global coherence while generating locally fluent text. Through extensive experiments on diverse long-form generation tasks including creative writing, technical reports, and narrative storytelling, we demonstrate that LongWriter-Zero achieves state-of-the-art performance, producing high-quality outputs exceeding 20,000 words while maintaining semantic consistency and factual accuracy throughout the generated content.'
image:
  preview_only: true
links:
  - name: arXiv
    url: https://arxiv.org/abs/2501.14724
  - name: PDF
    url: https://arxiv.org/pdf/2501.14724.pdf
  - name: Google Scholar
    url: https://scholar.google.com/scholar?q=LongWriter-Zero%3A%20Mastering%20Ultra-Long%20Text%20Generation%20via%20Reinforcement%20Learning
---


