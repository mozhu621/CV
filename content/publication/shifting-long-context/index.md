---
title: 'Shifting Long-Context LLMs Research from Input to Output'
authors:
- Yuhao Wu
- Yushi Bai
- Zhiqing Hu
- Shangqing Tu
- Ming Shan Hee
- Juanzi Li
- Roy Ka-Wei Lee
date: '2025-01-01'
publication_types:
- manuscript
publication: 'Submitted to ARR 2025'
abstract: 'Recent research in long-context large language models has predominantly focused on enhancing input processing capabilities, enabling models to handle increasingly longer context windows. In this work, we argue for a paradigm shift: directing research attention from long-context input to long-context output generation. We present a comprehensive study examining the unique challenges of generating lengthy, coherent outputs that maintain consistency and quality throughout. We propose novel evaluation metrics and benchmarks specifically designed for long-form generation, addressing issues such as content drift, repetition, and structural coherence. Our empirical analysis reveals that current models exhibit significant performance degradation in output quality as generation length increases, highlighting critical gaps in existing approaches and motivating future research directions in this underexplored area.'
image:
  preview_only: true
links:
  - name: arXiv
    url: https://arxiv.org/abs/2501.04975
  - name: PDF
    url: https://arxiv.org/pdf/2501.04975.pdf
  - name: Google Scholar
    url: https://scholar.google.com/scholar?q=Shifting%20Long-Context%20LLMs%20Research%20from%20Input%20to%20Output
---


