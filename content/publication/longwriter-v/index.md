---
title: 'LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models'
authors:
- Shangqing Tu
- Yucheng Wang
- Daniel Zhang-Li
- Yushi Bai
- Jifan Yu
- Yuhao Wu
- Lei Hou
- Huiqin Liu
- Zhiyuan Liu
- Bin Xu
- Juanzi Li
date: '2025-01-01'
publication_types:
- paper-conference
publication: 'ACM MM 2025'
abstract: 'Magnetic Tunnel Junctions (MTJs) have shown great promise as hardware sources for true random number generation (TRNG) due to their intrinsic stochastic switching behavior. However, practical deployment remains challenged by drift in switching probability caused by thermal fluctuations, device aging, and environmental instability. This work presents an engineering-oriented, drift-resilient MTJ-based TRNG architecture, enabled by a hybrid control strategy that combines self-stabilizing feedback with pulse width modulation. A key component is the Downcalibration-2 scheme, which updates the control parameter every two steps using only integer-resolution timing, ensuring excellent statistical quality without requiring bit discarding, pre-characterization, or external calibration. Extensive experimental measurements and numerical simulations demonstrate that this approach maintains stable randomness under dynamic temperature drift, using only simple digital logic. The proposed architecture offers high throughput, robustness, and scalability, making it well-suited for secure hardware applications, embedded systems, and edge computing environments.'
links:
  - name: arXiv
    url: https://arxiv.org/abs/2501.15206
  - name: PDF
    url: https://arxiv.org/pdf/2501.15206.pdf
  - name: Google Scholar
    url: https://scholar.google.com/scholar?q=LongWriter-V%3A%20Enabling%20Ultra-Long%20and%20High-Fidelity%20Generation%20in%20Vision-Language%20Models
---


