
@inproceedings{wu_sentistream_2023,
	location = {Singapore},
	title = {{SentiStream}: A Co-Training Framework for Adaptive Online Sentiment Analysis in Evolving Data Streams},
	url = {https://aclanthology.org/2023.emnlp-main.380},
	doi = {10.18653/v1/2023.emnlp-main.380},
	shorttitle = {{SentiStream}},
	eventtitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	pages = {6198--6212},
	booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Yuhao and Sharma, Karthick and Seah, Chun and Zhang, Shuhao},
	urldate = {2024-03-01},
	date = {2023},
	langid = {english},
	file = {全文:/Users/yuhao_wu/Zotero/storage/C9YHD27B/Wu 等 - 2023 - SentiStream A Co-Training Framework for Adaptive .pdf:application/pdf},
}

@misc{wu_online_2023,
	title = {Online Continual Knowledge Learning for Language Models},
	url = {http://arxiv.org/abs/2311.09632},
	abstract = {Large Language Models ({LLMs}) serve as repositories of extensive world knowledge, enabling them to perform tasks such as question-answering and fact-checking. However, this knowledge can become obsolete as global contexts change. In this paper, we introduce a novel problem in the realm of continual learning: Online Continual Knowledge Learning ({OCKL}). This problem formulation aims to manage the dynamic nature of world knowledge in {LMs} under real-time constraints. We propose a new benchmark and evaluation metric designed to measure both the rate of new knowledge acquisition and the retention of previously learned knowledge. Our empirical evaluation, conducted using a variety of state-of-the-art methods, establishes robust base-lines for {OCKL}. Our results reveal that existing continual learning approaches are unfortunately insufficient for tackling the unique challenges posed by {OCKL}. We identify key factors that influence the trade-off between knowledge acquisition and retention, thereby advancing our understanding of how to train {LMs} in a continually evolving environment.},
	number = {{arXiv}:2311.09632},
	publisher = {{arXiv}},
	author = {Wu, Yuhao and Shi, Tongjun and Sharma, Karthick and Seah, Chun Wei and Zhang, Shuhao},
	urldate = {2024-03-01},
	date = {2023-11-16},
	eprinttype = {arxiv},
	eprint = {2311.09632 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/yuhao_wu/Zotero/storage/G9LATUR3/Wu 等 - 2023 - Online Continual Knowledge Learning for Language M.pdf:application/pdf;arXiv.org Snapshot:/Users/yuhao_wu/Zotero/storage/W7LJYBFE/2311.html:text/html},
}
