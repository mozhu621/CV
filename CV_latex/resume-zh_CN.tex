\documentclass{resume}
\usepackage{zh_CN-Adobefonts_external} % Simplified Chinese Support using external fonts
\usepackage{linespacing_fix} % disable extra space before next section
\usepackage{cite}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue]{hyperref}

\begin{document}
\pagenumbering{gobble} % suppress displaying page number

\name{Yuhao Wu}

\basicInfo{
  \email{mozhu621@gmail.com} \textperiodcentered\
  \phone{(+65) 83777500 or (+86)17320504505 (Wechat)} \textperiodcentered
  % \google[Yuhao Wu]{https://scholar.google.com/citations?hl=en&user=XIyHTG0AAAAJ}
  }

\section{\faGraduationCap\ Education}
\datedsubsection{\textbf{Singapore University of Technology and Design (SUTD)}, Singapore}{2023 -- Present}
\textit{Ph.D. Student}\ in Natural Language Processing
\datedsubsection{\textbf{Tsinghua University (THU)}, Beijing, China}{Sep. 2024 -- Jul. 2025}
\textit{Visiting Ph.D. Student}\ at Knowledge Engineering Group (KEG)
\datedsubsection{\textbf{Huazhong Agricultural University}, Wuhan, Hubei, China}{2018 -- 2022}
\textit{Bachelor's Degree}\ in Mathematics

\section{\faUsers\ Internship Experience}
\datedsubsection{\textbf{Algorithm Research Intern at Moonshot AI (Kimi)} \qquad Beijing, China}{Jun. 2025 -- Present}
\begin{itemize}
    \item Deeply involved in the long-context capability iteration of Kimi-K2-0905, covering the construction of synthetic data for long text and long code generation.
    \item Advanced the enhancement of code capabilities, including building the evaluation system and optimizing the performance of Kimi-K2-0905 on Code Reasoning and Code Generation tasks.
\end{itemize}

\datedsubsection{\textbf{Algorithm Research Intern at Zhipu AI} \qquad Beijing, China}{Sep. 2024 -- Jun. 2025}
\begin{itemize}
    \item Deeply involved in the GLM-Zero series projects for the long-context RL enhancement of the GLM-4.1/4.5 model series. Work included long-chain CoT data construction, Reward Model design, RLHF alignment optimization, and end-to-end benchmark evaluation.
    \item Led the proposal of the O1/R1-think long-text generation framework, generating hierarchical SFT data via agent self-guidance and optimizing ultra-long text generation by combining hierarchical DPO with pure reinforcement learning strategies. Related work has been submitted to NeurIPS 2025 in two papers.
\end{itemize}

\datedsubsection{\textbf{NLP Algorithm Intern at Trip.com} \qquad Shanghai, China}{Jul. 2021 -- Oct. 2021}
\begin{itemize}
    \item Applied NLP and machine learning algorithms to mine and analyze text data in business scenarios, improving short-text matching performance by 9\%.
    \item Assisted in designing, mining, and constructing practical knowledge, knowledge reasoning, and intelligent dialogue response systems to support intelligent customer service. Authored a patent as the first author, which has been granted (CN Patent Number: 202111234433X).
\end{itemize}

\section{\faInfo\ Research Interests}
My research interests focus on Natural Language Processing, aiming to enhance the capabilities of existing Large Language Models (LLMs). Specifically, I address the shortcomings of current models in generating longer texts. My research is centered on the field of long-text generation, including but not limited to data generation, training algorithms (such as reinforcement learning), and evaluation methods.

\section{\faCogs\ Academic Research}
\begin{itemize}[parsep=0.5ex]
    \item \textbf{Co-authors} "\href{https://scholar.google.com/scholar?q=GLM-4.5%3A+Agentic%2C+Reasoning%2C+and+Coding+%28ARC%29+Foundation+Models}{GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models}" Technology Report.
    \item \textbf{Yuhao Wu*}, Yushi Bai*, Jiajie Zhang, Xin lv, Roy Ka-Wei Lee, Juanzi Li, “\href{https://scholar.google.com/scholar?q=LongWriter-Zero%3A+Mastering+Ultra-Long+Text+Generation+via+Reinforcement+Learning}{LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning}” submitted to ICLR 2025.
    \item \textbf{Yuhao Wu*}, Yushi Bai*, Zhiqing Hu, Juanzi Li, Roy Ka-Wei Lee, “\href{https://scholar.google.com/scholar?q=SuperWriter%3A+Reflection-Driven+Long-Form+Writing+with+LLMs}{SuperWriter: Reflection-Driven Long-Form Writing with LLMs}” submitted to ARR 2025.
    \item \textbf{Yuhao Wu}, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee, “\href{https://scholar.google.com/scholar?q=Shifting+Long-Context+LLMs+Research+from+Input+to+Output}{Shifting Long-Context LLMs Research from Input to Output}” submitted to ARR 2025.
    \item Shangqing Tu, Yucheng Wang, Daniel Zhang-Li, Yushi Bai, Jifan Yu, \textbf{Yuhao Wu}, Lei Hou, Huiqin Liu, Zhiyuan Liu, Bin Xu, and Juanzi Li, “\href{https://scholar.google.com/scholar?q=LongWriter-V%3A+Enabling+Ultra-Long+and+High-Fidelity+Generation+in+Vision-Language+Models}{LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models}”, ACM MM 2025.
    \item Ziyu Ge*, \textbf{Yuhao Wu*}, Daniel Chin, Roy Ka-Wei Lee, Rui Cao, “\href{https://scholar.google.com/scholar?q=Resolving+Conflicting+Evidence+in+Automated+Fact-Checking%3A+A+Study+on+Retrieval-Augmented+LLMs}{Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs}” in \textbf{IJCAI 2025}.
    \item \textbf{Yuhao Wu}, Ming Shan Hee, Zhiqing Hu, Roy Ka-Wei Lee, “\href{https://scholar.google.com/scholar?q=LongGenBench%3A+Benchmarking+Long-Form+Generation+in+Long+Context+LLMs}{LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs}” in \textbf{ICLR 2025 main track}.
    \item \textbf{Yuhao Wu*}, T. Shi*, K. Sharma, C. W. Seah, and S. Zhang, “\href{https://arxiv.org/abs/2311.09632}{Online Continual Knowledge Learning for Language Models}”.
    \item \textbf{Yuhao Wu}, K. Sharma, C. Seah, and S. Zhang, “\href{https://aclanthology.org/2023.emnlp-main.380}{SentiStream: A Co-Training Framework for Adaptive Online Sentiment Analysis in Evolving Data Streams}” in \textbf{EMNLP 2023 main track}.
\end{itemize}

\section{\faHeartO\ Awards}
\datedline{\textit{National First Prize}, 2020 Higher Education Cup National Mathematical Modeling Contest. Served as team captain, responsible for main modeling and programming work.}{Sep. 2020}
\datedline{\textit{National First Prize}, The 10th MathorCup University Mathematical Modeling Challenge. Advanced to the post-competition research phase (top 3 teams nationwide).}{May 2020}

\end{document}
